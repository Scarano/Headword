run-under-name: [10-24-nosrilm]

overwrite-output-dir: false

#data-root: /scratch/scarano.s/data/v4
#temp-root: /tmp/scarano.s
#srilm.bin: /scratch/scarano.s/src/srilm/bin/i686-m64

data-root: /Users/sam/thesis-data/v4
temp-root: /Users/sam/thesis-data/temp
srilm.bin: /Users/sam/src/srilm/bin/macosx

output-root: /Users/sam/thesis-data/output

#test-set.name: [reflectionsuponm00aste appealinfavor00child saratogaatalere00cushgoog saratogaatalere01cushgoog anaccountameric01judsgoog fallriverauthent01will]
#test-set.name: [appealinfavor00child saratogaatalere00cushgoog anaccountameric01judsgoog]
test-set.name: [reflectionsuponm00aste]
test-set.file: ocr-corpus/aligned/${test-set.name}.salign

test-set.min-sentence-length: 1
test-set.max-sentence-length: [15]

training-set: [1to30]

#dictionary.file: wwp/prose2.train.vocab.txt
dictionary.max-matches: 100

channel.model-dir: channelmodel
channel.model-em-steps: 8
channel.model: ${channel.model-dir}/model-fix1.${channel.model-em-steps}.javaobj
channel.smoothing.repro-prob: 0.9
channel.smoothing.subst-prob: 0.0001
channel.smoothing.ins-prob-factor: 1
channel.smoothing.del-prob-factor: 1
channel.smoothing.lambda: 0.95
channel.beam-width: 3
channel.weight: 1

# lm.sentence-candidates is the "n" in n-best. If 0, n-best rescoring is not performed.
# If > 0, SRILMNBestCorrector is used. Setting to 0 or 1 should result in the same behavior
# (but not the same execution time).
# -1 means skip SRILM, just pass the lattice directly to the parser
lm.sentence-candidates: [-1]
lm.order: 3
#srilm.variant: [kni.v3 wb.v2]
srilm.variant: 4kni
srilm.unk-prob: 6
srilm.set: prose.${training-set}
srilm.model: srilm/${srilm.set}.lm.${srilm.variant}-unk${srilm.unk-prob}.txt

lattice.word-candidates: [10]
lattice.max-word-merges: 1
lattice.max-punctuation-merges: 3
lattice.allow-free-dehyphenation: true

clustering.n: [100]
clustering.file: clustering/prose.train.deh.lc-c${clustering.n}-p1.out/paths
clustering.unk-prob: 1e-${srilm.unk-prob}

#rescore.method: [tag-oracle]
#rescore.method: [parse]
#rescore.method: tag-ngram
rescore.method: lex-parse
rescore.log-linear-mix: false
rescore.normalize: T
rescore.weight: 0.5

#tag-ngram.model: tagngram/prose.train.clean1.c${clustering.n}.${clustering.caseless}.${clustering.punctuation}.lm.7wb.txt
#tag-ngram.order: [3]
#tag-ngram.generate-fully: true

#parser.grammar.training-set: prose.train.clean1.4to10.c${clustering.n}.${clustering.caseless}
#parser.grammar.variant: ip.model70
#parser.grammar.name: ${parser.grammar.training-set}.${parser.grammar.variant}
#parser.grammar.file: grammar/models/${parser.grammar.name}.gra
#parser.cache.file: cache/parses_${parser.grammar.name}.${test-set.max-sentence-length}

parser.model-training: 4to10
parser.model-method: [em.har]
parser.model-family: grammar/model.prose.${parser.model-training}.c${clustering.n}.${parser.model-method}
parser.model-lex-variant: tagalpha1.v
parser.tag-model-file: ${parser.model-family}.cnt
parser.tag-model-alpha: [1]
parser.lex-model-file: ${parser.model-family}.${training-set}.${parser.model-lex-variant}.lcnt
parser.lex-model-alpha: [-1]
parser.lex-model-lambda: [0.5]
parser.lex-unk-prob: [1e-9 1e-10]
parser.combined-model: T
parser.marginal-probability: T


output-file: ${test-set.name}.corr
